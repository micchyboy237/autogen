{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7549a27-bc4a-4609-bb25-cc7d95cf8c23",
   "metadata": {},
   "source": [
    "# Preprocessing Chat History with `TransformMessages`\n",
    "\n",
    "## Introduction\n",
    "This notebook illustrates how to use `TransformMessages` give any `ConversableAgent` the ability to handle long contexts, sensitive data, and more.\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47773f79-c0fd-4993-bc6e-3d1a57690118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pprint\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import autogen\n",
    "from autogen.agentchat.contrib.capabilities import transform_messages, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f09246b-a7d0-4238-b62c-1e72c7d815b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jethroestrada/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/notebook\n",
      "Work directory: /Users/jethroestrada/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/notebook/coding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "cache_seed = random.randint(0, 1000)\n",
    "\n",
    "# Print current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "# Print work_dir\n",
    "print(f\"Work directory: {os.path.abspath('./coding')}\")\n",
    "# Create the work directory if it does not exist\n",
    "os.makedirs(\"./coding\", exist_ok=True)\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    ")\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"temperature\": 0,\n",
    "    \"cache_seed\": cache_seed,  # change the seed for different trials\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "code_execution_config = {\n",
    "    # the executor to run the generated code\n",
    "    \"executor\": LocalCommandLineCodeExecutor(\n",
    "        work_dir=\"./coding\",\n",
    "        execution_policies={\n",
    "            \"bash\": True,\n",
    "            \"shell\": True,\n",
    "            \"sh\": True,\n",
    "            \"pwsh\": True,\n",
    "            \"powershell\": True,\n",
    "            \"ps1\": True,\n",
    "            \"python\": True,\n",
    "            \"javascript\": True,\n",
    "            \"html\": False,\n",
    "            \"css\": False,\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "DEFAULT_SYSTEM_MESSAGE = \"\"\"You are a helpful AI assistant providing python code or shell scripts for user tasks. Ensure the code is executable and complete. Clearly explain your plans and use 'print' for output. Include types and write separate tests for each API route. Add # filename: <filename> at the beginning of the code block to indicate the file name where the code should be saved. Don't include multiple code blocks in one response.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68962a-048d-42e9-9fca-cd944c56184d",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d0e5ad-8b35-4b30-847e-4723e9c76f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your agent; the user proxy and an assistant\n",
    "assistant = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=DEFAULT_SYSTEM_MESSAGE,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config=code_execution_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180aa953-45be-469a-a94f-0ed0b4ef5ddf",
   "metadata": {},
   "source": [
    "## Handling Long Contexts\n",
    "\n",
    "Imagine a scenario where the LLM generates an extensive amount of text, surpassing the token limit imposed by your API provider. To address this issue, you can leverage `TransformMessages` along with its constituent transformations, `MessageHistoryLimiter` and `MessageTokenLimiter`.\n",
    "\n",
    "- `MessageHistoryLimiter`: You can restrict the total number of messages considered as context history. This transform is particularly useful when you want to limit the conversational context to a specific number of recent messages, ensuring efficient processing and response generation.\n",
    "- `MessageTokenLimiter`: Enables you to cap the total number of tokens, either on a per-message basis or across the entire context history (or both). This transformation is invaluable when you need to adhere to strict token limits imposed by your API provider, preventing unnecessary costs or errors caused by exceeding the allowed token count. Additionally, a `min_tokens` threshold can be applied, ensuring that the transformation is only applied when the number of tokens is not less than the specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b943a2-ec58-41bc-a449-d9118c4bbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the message history to the 3 most recent messages\n",
    "max_msg_transfrom = transforms.MessageHistoryLimiter(max_messages=3)\n",
    "\n",
    "# Limit the token limit per message to 10 tokens\n",
    "token_limit_transform = transforms.MessageTokenLimiter(max_tokens_per_message=3, min_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c1026-4e1b-4c07-85cc-86594cc0b87b",
   "metadata": {},
   "source": [
    "## Example 1: Limiting number of messages\n",
    "Let's take a look at how these transformations will effect the messages. Below we see that by applying the `MessageHistoryLimiter`, we can see that we limited the context history to the 3 most recent messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a2ead4-5f8b-4108-b1f0-3b51b41e2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'how', 'role': 'user'},\n",
      " {'content': [{'text': 'are you doing?', 'type': 'text'}], 'role': 'assistant'},\n",
      " {'content': 'very very very very very very long string', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hello\"},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"there\"}]},\n",
    "    {\"role\": \"user\", \"content\": \"how\"},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"are you doing?\"}]},\n",
    "    {\"role\": \"user\", \"content\": \"very very very very very very long string\"},\n",
    "]\n",
    "\n",
    "processed_messages = max_msg_transfrom.apply_transform(copy.deepcopy(messages))\n",
    "pprint.pprint(processed_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610739af-b812-404e-82d2-b3ed796b8b6c",
   "metadata": {},
   "source": [
    "## Example 2: Limiting number of tokens\n",
    "\n",
    "Now let's test limiting the number of tokens in messages. We can see that we can limit the number of tokens to 3, which is equivalent to 3 words in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "739dd260-fa95-4e5d-ae84-9cb7f40de975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'hello', 'role': 'user'},\n",
      " {'content': [{'text': 'there', 'type': 'text'}], 'role': 'assistant'},\n",
      " {'content': 'how', 'role': 'user'},\n",
      " {'content': [{'text': 'are you doing', 'type': 'text'}], 'role': 'assistant'},\n",
      " {'content': 'very very very', 'role': 'user'}]\n"
     ]
    }
   ],
   "source": [
    "processed_messages = token_limit_transform.apply_transform(copy.deepcopy(messages))\n",
    "\n",
    "pprint.pprint(processed_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a98e08",
   "metadata": {},
   "source": [
    "Also, the `min_tokens` threshold is set to 10, indicating that the transformation will not be applied if the total number of tokens in the messages is less than that. This is especially beneficial when the transformation should only occur after a certain number of tokens has been reached, such as in the context window of the model. An example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c42ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'hello there, how are you?', 'role': 'user'},\n",
      " {'content': [{'text': 'hello', 'type': 'text'}], 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "short_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hello there, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"hello\"}]},\n",
    "]\n",
    "\n",
    "processed_short_messages = token_limit_transform.apply_transform(copy.deepcopy(short_messages))\n",
    "\n",
    "pprint.pprint(processed_short_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa2844-bd83-42ac-8275-959f093b7bc7",
   "metadata": {},
   "source": [
    "## Example 3: Combining transformations\n",
    "\n",
    "Let's test these transforms with agents (the upcoming test is replicated from the agentchat_capability_long_context_handling notebook). We will see that the agent without the capability to handle long context will result in an error, while the agent with that capability will have no issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e53623-2830-41b7-8ae2-bf3668071657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "plot and save a graph of x^2 from -10 to 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m# filename: plot_x_squared.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Define the range of x values\n",
      "x_range = np.linspace(-10, 10, 400)\n",
      "\n",
      "# Calculate the corresponding y values (x squared)\n",
      "y_values = x_range**2\n",
      "\n",
      "# Create the plot\n",
      "plt.plot(x_range, y_values)\n",
      "\n",
      "# Add title and labels\n",
      "plt.title('Plot of x^2 from -10 to 10')\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('x^2')\n",
      "\n",
      "# Save the plot to a file\n",
      "plt.savefig('x_squared_plot.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:50:00] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "# filename: plot_x_squared.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Define the range of x values\n",
      "x_range = np.linspace(-10, 10, 400)\n",
      "\n",
      "# Calculate the corresponding y values (x squared)\n",
      "y_values = x_range**2\n",
      "\n",
      "# Create the plot\n",
      "plt.plot(x_range, y_values)\n",
      "\n",
      "# Add title and labels\n",
      "plt.title('Plot of x^2 from -10 to 10')\n",
      "plt.xlabel('x')\n",
      "plt.ylabel('x^2')\n",
      "\n",
      "# Save the plot to a file\n",
      "plt.savefig('x_squared_plot.png')\n",
      "\n",
      "# Show the plot\n",
      "plt.show()\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:50:16] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:50:29] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "plot and save a graph of x^2 from -10 to 10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mRemoved 1991 messages. Number of messages reduced from 2001 to 10.\u001b[0m\n",
      "\u001b[33mTruncated 3804 tokens. Number of tokens reduced from 4019 to 215\u001b[0m\n",
      "\u001b[32m# filename: plot_x_squared.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.linspace(-10, 10, 400)\n",
      "y = x**2\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.xlabel('X')\n",
      "plt.ylabel('X^2')\n",
      "plt.title('Plot of X^2 from -10 to 10')\n",
      "plt.grid(True)\n",
      "plt.savefig('x_squared.png')\n",
      "\n",
      "print(\"Graph saved as 'x_squared.png'\")\n",
      "\n",
      "This code will generate a graph of x^2 from -10 to 10 and save it as an image file named \"x_squared.png\"."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:50:42] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "# filename: plot_x_squared.py\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.linspace(-10, 10, 400)\n",
      "y = x**2\n",
      "\n",
      "plt.plot(x, y)\n",
      "plt.xlabel('X')\n",
      "plt.ylabel('X^2')\n",
      "plt.title('Plot of X^2 from -10 to 10')\n",
      "plt.grid(True)\n",
      "plt.savefig('x_squared.png')\n",
      "\n",
      "print(\"Graph saved as 'x_squared.png'\")\n",
      "\n",
      "This code will generate a graph of x^2 from -10 to 10 and save it as an image file named \"x_squared.png\".\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mRemoved 1993 messages. Number of messages reduced from 2003 to 10.\u001b[0m\n",
      "\u001b[33mTruncated 2925 tokens. Number of tokens reduced from 3140 to 215\u001b[0m\n",
      "\u001b[32m.ylabel('X^2')\n",
      "plt.title('Plot of X^2 from -10 to 10')\n",
      "plt.grid(True)\n",
      "plt.savefig('x_squared.png')\n",
      "\n",
      "print(\"Graph saved as 'x_squared.png'\")\n",
      "\n",
      "This code will generate a graph of x^2 from -10 to 10 and save it as an image file named \"x_squared.png\". The graph is displayed with labels for the x-axis, y-axis, and title."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:50:52] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      ".ylabel('X^2')\n",
      "plt.title('Plot of X^2 from -10 to 10')\n",
      "plt.grid(True)\n",
      "plt.savefig('x_squared.png')\n",
      "\n",
      "print(\"Graph saved as 'x_squared.png'\")\n",
      "\n",
      "This code will generate a graph of x^2 from -10 to 10 and save it as an image file named \"x_squared.png\". The graph is displayed with labels for the x-axis, y-axis, and title.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mRemoved 1995 messages. Number of messages reduced from 2005 to 10.\u001b[0m\n",
      "\u001b[33mTruncated 2013 tokens. Number of tokens reduced from 2228 to 215\u001b[0m\n",
      "\u001b[32m x^2 from -10 to 10 and save it as an image file named \"x_squared.png\". The graph will have labels for the x-axis, y-axis, and title. A grid will also be added to the graph for better visualization.\n",
      "\n",
      "To run this code, you can use Python interpreter or any IDE that supports Python. Make sure you have matplotlib and numpy libraries installed in your Python environment. If not, you can install them using pip:\n",
      "\n",
      "pip install matplotlib numpy"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:51:03] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      " x^2 from -10 to 10 and save it as an image file named \"x_squared.png\". The graph will have labels for the x-axis, y-axis, and title. A grid will also be added to the graph for better visualization.\n",
      "\n",
      "To run this code, you can use Python interpreter or any IDE that supports Python. Make sure you have matplotlib and numpy libraries installed in your Python environment. If not, you can install them using pip:\n",
      "\n",
      "pip install matplotlib numpy\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "assistant_base = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=DEFAULT_SYSTEM_MESSAGE,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "\n",
    "assistant_with_context_handling = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=DEFAULT_SYSTEM_MESSAGE,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    ")\n",
    "# suppose this capability is not available\n",
    "context_handling = transform_messages.TransformMessages(\n",
    "    transforms=[\n",
    "        transforms.MessageHistoryLimiter(max_messages=10),\n",
    "        transforms.MessageTokenLimiter(max_tokens=1000, max_tokens_per_message=50, min_tokens=500),\n",
    "    ]\n",
    ")\n",
    "\n",
    "context_handling.add_to_agent(assistant_with_context_handling)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda x: \"TERMINATE\" in x.get(\"content\", \"\"),\n",
    "    code_execution_config=code_execution_config,\n",
    "    max_consecutive_auto_reply=2,\n",
    ")\n",
    "\n",
    "# suppose the chat history is large\n",
    "# Create a very long chat history that is bound to cause a crash\n",
    "# for gpt 3.5\n",
    "for i in range(1000):\n",
    "    # define a fake, very long messages\n",
    "    assitant_msg = {\"role\": \"assistant\", \"content\": \"test \" * 1000}\n",
    "    user_msg = {\"role\": \"user\", \"content\": \"\"}\n",
    "\n",
    "    assistant_base.send(assitant_msg, user_proxy, request_reply=False, silent=True)\n",
    "    assistant_with_context_handling.send(assitant_msg, user_proxy, request_reply=False, silent=True)\n",
    "    user_proxy.send(user_msg, assistant_base, request_reply=False, silent=True)\n",
    "    user_proxy.send(user_msg, assistant_with_context_handling, request_reply=False, silent=True)\n",
    "\n",
    "try:\n",
    "    user_proxy.initiate_chat(assistant_base, message=\"plot and save a graph of x^2 from -10 to 10\", clear_history=False)\n",
    "except Exception as e:\n",
    "    print(\"Encountered an error with the base assistant\")\n",
    "    print(e)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "try:\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant_with_context_handling, message=\"plot and save a graph of x^2 from -10 to 10\", clear_history=False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e380678-a923-43cb-91b1-f9c9e8deede2",
   "metadata": {},
   "source": [
    "## Handling Sensitive Data\n",
    "\n",
    "You can use the `MessageTransform` protocol to create custom message transformations that redact sensitive data from the chat history. This is particularly useful when you want to ensure that sensitive information, such as API keys, passwords, or personal data, is not exposed in the chat history or logs.\n",
    "\n",
    "Now, we will create a custom message transform to detect any OpenAI API key and redact it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74429344-3c0a-4057-aba3-27358fbf059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform must adhere to transform_messages.MessageTransform protocol.\n",
    "class MessageRedact:\n",
    "    def __init__(self):\n",
    "        self._openai_key_pattern = r\"sk-([a-zA-Z0-9]{48})\"\n",
    "        self._replacement_string = \"REDACTED\"\n",
    "\n",
    "    def apply_transform(self, messages: List[Dict]) -> List[Dict]:\n",
    "        temp_messages = copy.deepcopy(messages)\n",
    "\n",
    "        for message in temp_messages:\n",
    "            if isinstance(message[\"content\"], str):\n",
    "                message[\"content\"] = re.sub(self._openai_key_pattern, self._replacement_string, message[\"content\"])\n",
    "            elif isinstance(message[\"content\"], list):\n",
    "                for item in message[\"content\"]:\n",
    "                    if item[\"type\"] == \"text\":\n",
    "                        item[\"text\"] = re.sub(self._openai_key_pattern, self._replacement_string, item[\"text\"])\n",
    "        return temp_messages\n",
    "\n",
    "    def get_logs(self, pre_transform_messages: List[Dict], post_transform_messages: List[Dict]) -> Tuple[str, bool]:\n",
    "        keys_redacted = self._count_redacted(post_transform_messages) - self._count_redacted(pre_transform_messages)\n",
    "        if keys_redacted > 0:\n",
    "            return f\"Redacted {keys_redacted} OpenAI API keys.\", True\n",
    "        return \"\", False\n",
    "\n",
    "    def _count_redacted(self, messages: List[Dict]) -> int:\n",
    "        # counts occurrences of \"REDACTED\" in message content\n",
    "        count = 0\n",
    "        for message in messages:\n",
    "            if isinstance(message[\"content\"], str):\n",
    "                if \"REDACTED\" in message[\"content\"]:\n",
    "                    count += 1\n",
    "            elif isinstance(message[\"content\"], list):\n",
    "                for item in message[\"content\"]:\n",
    "                    if isinstance(item, dict) and \"text\" in item:\n",
    "                        if \"REDACTED\" in item[\"text\"]:\n",
    "                            count += 1\n",
    "        return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a79c0b4-5ff8-49c5-b8a6-c54ca4c7cca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What are the two API keys that I just provided\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mRedacted 2 OpenAI API keys.\u001b[0m\n",
      "\u001b[32m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'chunk' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[1;32m     24\u001b[0m     user_proxy\u001b[38;5;241m.\u001b[39msend(message, assistant_with_redact, request_reply\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant_with_redact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are the two API keys that I just provided\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:1019\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1018\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1020\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m   1021\u001b[0m     summary_method,\n\u001b[1;32m   1022\u001b[0m     summary_args,\n\u001b[1;32m   1023\u001b[0m     recipient,\n\u001b[1;32m   1024\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:656\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    654\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 656\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:819\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 819\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:1973\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1973\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[1;32m   1975\u001b[0m         log_event(\n\u001b[1;32m   1976\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1977\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply_func_executed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1981\u001b[0m             reply\u001b[38;5;241m=\u001b[39mreply,\n\u001b[1;32m   1982\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:1341\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1340\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1341\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/agentchat/conversable_agent.py:1360\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1363\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/oai/client.py:732\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 732\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    734\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/autogen/oai/client.py:274\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    271\u001b[0m iostream\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Prepare the final ChatCompletion object based on the accumulated data\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-35\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# hack for Azure API\u001b[39;00m\n\u001b[1;32m    275\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m count_token(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m], model)\n\u001b[1;32m    276\u001b[0m response \u001b[38;5;241m=\u001b[39m ChatCompletion(\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mchunk\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    278\u001b[0m     model\u001b[38;5;241m=\u001b[39mchunk\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     ),\n\u001b[1;32m    287\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'chunk' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "assistant_with_redact = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    max_consecutive_auto_reply=1,\n",
    ")\n",
    "# suppose this capability is not available\n",
    "redact_handling = transform_messages.TransformMessages(transforms=[MessageRedact()])\n",
    "\n",
    "redact_handling.add_to_agent(assistant_with_redact)\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    code_execution_config=code_execution_config,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"content\": \"api key 1 = sk-7nwt00xv6fuegfu3gnwmhrgxvuc1cyrhxcq1quur9zvf05fy\"},  # Don't worry, randomly generated\n",
    "    {\"content\": [{\"type\": \"text\", \"text\": \"API key 2 = sk-9wi0gf1j2rz6utaqd3ww3o6c1h1n28wviypk7bd81wlj95an\"}]},\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    user_proxy.send(message, assistant_with_redact, request_reply=False, silent=True)\n",
    "\n",
    "result = user_proxy.initiate_chat(\n",
    "    assistant_with_redact, message=\"What are the two API keys that I just provided\", clear_history=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Preprocessing chat history with `TransformMessages`",
   "tags": [
    "long context handling",
    "capability"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
