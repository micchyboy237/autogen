{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Writing a software application using function calls\n",
    "\n",
    "The default way of creating code in Autogen is its built-in code extractor. Although it allows for creating and executing simple scripts fast, that way of creating code is not suitable for developing advanced software applications, according to my experiences. The process of developing an application is mostly the process of introducing changes into existing files rather than creating new files with code. And in my experience, the code extractor is bad at introducing changes as the model often gets lost and can damage existing files.\n",
    "\n",
    "Properly created functions that can modify code provide us with the ability to have more control over code changes and result in better quality. Additionally, as the scope of possible operations is predefined inside the tools, we can safely use Autogen without Docker, avoiding all the complications related to it.\n",
    "\n",
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c528cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyautogen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Set your API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca301a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/jethroestrada/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/notebook\n",
      "Work directory: /Users/jethroestrada/Desktop/External_Projects/GPT/ResumeChatbot/autogen_jet/notebook/coding\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "import os\n",
    "import random\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "cache_seed = random.randint(0, 1000)\n",
    "\n",
    "# Print current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "# Print work_dir\n",
    "print(f\"Work directory: {os.path.abspath('./coding')}\")\n",
    "# Create the work directory if it does not exist\n",
    "os.makedirs(\"./coding\", exist_ok=True)\n",
    "\n",
    "config_list = autogen.config_list_from_json(\"OAI_CONFIG_LIST\")\n",
    "\n",
    "llm_config = {\n",
    "    \"timeout\": 600,\n",
    "    \"cache_seed\": cache_seed,  # change the seed for different trials\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "}\n",
    "\n",
    "code_execution_config = {\n",
    "    # the executor to run the generated code\n",
    "    \"executor\": LocalCommandLineCodeExecutor(\n",
    "        work_dir=\"./coding\",\n",
    "        execution_policies={\n",
    "            \"bash\": True,\n",
    "            \"shell\": True,\n",
    "            \"sh\": True,\n",
    "            \"pwsh\": True,\n",
    "            \"powershell\": True,\n",
    "            \"ps1\": True,\n",
    "            \"python\": True,\n",
    "            \"javascript\": True,\n",
    "            \"html\": False,\n",
    "            \"css\": False,\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "DEFAULT_SYSTEM_MESSAGE = \"\"\"You are a helpful AI assistant providing python code or shell scripts for user tasks. Ensure the code is executable and complete. Clearly explain your plans and use 'print' for output. Include types and write separate tests for each API route. Add # filename: <filename> at the beginning of the code block to indicate the file name where the code should be saved. Don't include multiple code blocks in one response.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Create agents\n",
    "\n",
    "In this example, we will improve a simple FastAPI application using only dedicated function calls. Let's create an Engineer agent that will think out and execute code changes, and a user proxy Admin agent, through which we will guide our Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a10c9fe-1fbc-40c6-b655-5d2256864ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=DEFAULT_SYSTEM_MESSAGE,\n",
    "    # system_message=\"\"\"\n",
    "    # I'm Engineer. I'm expert in python programming. I'm executing code tasks required by Admin.\n",
    "    # \"\"\",\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    code_execution_config=code_execution_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "966c96a4-cc8a-4400-b8db-a21b7142e33c",
   "metadata": {},
   "source": [
    "Mention, unlike in many other examples, here we don't need a separate Executor agent to save our code, as that will be done by functions. We also don't need Docker to be running because of that - which makes the entire process easier.\n",
    "\n",
    "Next, let's set up our group chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354b4a8f-7a96-455b-9f17-cbc19d880462",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[engineer, user_proxy],\n",
    "    messages=[],\n",
    "    max_round=500,\n",
    "    speaker_selection_method=\"round_robin\",\n",
    "    enable_clear_history=True,\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7b0ad4c-a287-456d-9c0e-c4895e5f8ed2",
   "metadata": {},
   "source": [
    "## Prepare appropriate functions\n",
    "\n",
    "Let's go to core of the thing. Prepare functions that provide Engineer with functionality to modify existing code, create new code files, check filesystem and files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94b85d81-bdc5-4c9c-a9da-59a796317731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of the function 'list_dir' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n",
      "The return type of the function 'see_file' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n",
      "The return type of the function 'modify_code' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n",
      "The return type of the function 'create_file_with_code' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated\n",
    "\n",
    "default_path = \"backend_dir/\"\n",
    "# Create the backend directory if it does not exist\n",
    "os.makedirs(default_path, exist_ok=True)\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@engineer.register_for_llm(description=\"List files in choosen directory.\")\n",
    "def list_dir(directory: Annotated[str, \"Directory to check.\"]):\n",
    "    files = os.listdir(default_path + directory)\n",
    "    return 0, files\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@engineer.register_for_llm(description=\"Check the contents of a chosen file.\")\n",
    "def see_file(filename: Annotated[str, \"Name and path of file to check.\"]):\n",
    "    with open(default_path + filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    formatted_lines = [f\"{i+1}:{line}\" for i, line in enumerate(lines)]\n",
    "    file_contents = \"\".join(formatted_lines)\n",
    "\n",
    "    return 0, file_contents\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@engineer.register_for_llm(description=\"Replace old piece of code with new one. Proper indentation is important.\")\n",
    "def modify_code(\n",
    "    filename: Annotated[str, \"Name and path of file to change.\"],\n",
    "    start_line: Annotated[int, \"Start line number to replace with new code.\"],\n",
    "    end_line: Annotated[int, \"End line number to replace with new code.\"],\n",
    "    new_code: Annotated[str, \"New piece of code to replace old code with. Remember about providing indents.\"],\n",
    "):\n",
    "    with open(default_path + filename, \"r+\") as file:\n",
    "        file_contents = file.readlines()\n",
    "        file_contents[start_line - 1 : end_line] = [new_code + \"\\n\"]\n",
    "        file.seek(0)\n",
    "        file.truncate()\n",
    "        file.write(\"\".join(file_contents))\n",
    "    return 0, \"Code modified\"\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution()\n",
    "@engineer.register_for_llm(description=\"Create a new file with code.\")\n",
    "def create_file_with_code(\n",
    "    filename: Annotated[str, \"Name and path of file to create.\"], code: Annotated[str, \"Code to write in the file.\"]\n",
    "):\n",
    "    with open(default_path + filename, \"w\") as file:\n",
    "        file.write(code)\n",
    "    return 0, \"File created successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a09c9",
   "metadata": {},
   "source": [
    "## Prepare code to work with\n",
    "\n",
    "In this example, we will show how AI can extend the functionalities of existing code, as improving existing code is a much more frequent use case in software development than creating a new one. Let's prepare the initial code on which we will work. That will be a simple FastAPI script that will allow you to calculate today's stock spread in percentage for CD Project Red, a famous Polish gamedev company. Create a folder called 'backend_dir' and place a 'main.py' file here with the following content:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a9f8d-d5ce-4127-8646-cf0e4effd9f5",
   "metadata": {},
   "source": [
    "```python\n",
    "# backend_dir/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "import yfinance as yf\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/cdr_daily_spread\")\n",
    "async def calculate_daily_spread():\n",
    "    cdr = yf.Ticker(\"CDR.WA\")\n",
    "    today_data = cdr.history(period=\"1d\")\n",
    "    spread = ((today_data[\"High\"] - today_data[\"Low\"]) / today_data[\"Low\"]) * 100\n",
    "    return spread\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945003b5-b764-4ef1-99d9-b9464e39dfed",
   "metadata": {},
   "source": [
    "Install needed libraries. We can run our API using:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Send a request to 'localhost:8000/cdr_daily_spread' to check if it works.\n",
    "\n",
    "## Edit code using agents\n",
    "\n",
    "Let's assume we want our agents to extend the functionality of the application. Let's modify the endpoint to check the spread also for 11bits, another gamedev company, and compare it for both stocks. Also, let's separate the internal logic into a different file.\n",
    "\n",
    "Finally, instantiate a chat between the Engineer and the Admin. It will start by exploring the filesystem first, and after that, it will wait for our orders. Then, we will explain the task to the Engineer and ask him to provide a plan of changes first - according to my experience, that greatly increases the quality of LLM responses.\n",
    "\n",
    "After that, introduce changes with the Engineer one after another. Ask him to correct something or improve the functionality if needed. Do not hesitate to interrupt the tool's execution if you feel he is going to do something wrong. If errors occur, provide him with the error log and ask him to check out the file to refresh his knowledge about it before actually introducing changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4458a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5518947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "You will need to improve app in FastApi. For now, check out all the application files, try to understand it and wait for next instructions.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:34:12] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_8c2eccfa-cadc-4d5e-9936-3b8feaae9e17): understand_app *****\u001b[0m\n",
      "Arguments: \n",
      "{\"app_files\": [\"main.py\", \"models.py\", \"routes.py\", \"utils.py\"]}\n",
      "\u001b[32m*******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_8c2eccfa-cadc-4d5e-9936-3b8feaae9e17) *****\u001b[0m\n",
      "Error: Function understand_app not found.\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:34:16] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_0255c5f9-da93-47c9-b6ca-663aa1007311): understand_app *****\u001b[0m\n",
      "Arguments: \n",
      "{\"app_files\": \"all application files\"}\n",
      "\u001b[32m*******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_0255c5f9-da93-47c9-b6ca-663aa1007311) *****\u001b[0m\n",
      "Error: Function understand_app not found.\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:35:08] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_0fe92f1f-c0c3-471f-9b9f-e6cf67be842b): understand_app *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m*******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_0fe92f1f-c0c3-471f-9b9f-e6cf67be842b) *****\u001b[0m\n",
      "Error: Function understand_app not found.\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:35:17] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_36064ba9-a592-4dc0-ae86-e59a749e1ba8): understand_app *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m*******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_36064ba9-a592-4dc0-ae86-e59a749e1ba8) *****\u001b[0m\n",
      "Error: Function understand_app not found.\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:35:26] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_e71d6652-10ab-4173-8f4d-c73d53294699): understand_app *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m*******************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_e71d6652-10ab-4173-8f4d-c73d53294699) *****\u001b[0m\n",
      "USER INTERRUPTED\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "***** Response from calling tool \"call_IVJNPI12s4fCzysnWjExZjL2\" ***** [0, \"1:from fastapi import FastAPI\\n2:import yfinance as yf\\n3:\\n4:app = FastAPI()\\n5:\\n6:@app.get(\\\"/cdr_daily_spread\\\")\\n7:async def calculate_daily_spread():\\n8:    cdr = yf.Ticker(\\\"CDR.WA\\\")\\n9:    today_data = cdr.history(period=\\\"1d\\\")\\n10:    spread = ((today_data[\\\"High\\\"] - today_data[\\\"Low\\\"]) / today_data[\\\"Low\\\"]) * 100\\n11:    return spread\"] **********************************************************************\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:38:19] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_70fb123d-cb48-411d-bcf6-06459f7ffa05): calculate_daily_spread *****\u001b[0m\n",
      "Arguments: \n",
      "{\"period\": \"1d\"}\n",
      "\u001b[32m***************************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_70fb123d-cb48-411d-bcf6-06459f7ffa05) *****\u001b[0m\n",
      "Error: Function calculate_daily_spread not found.\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Engineer\n",
      "\u001b[0m\n",
      "\u001b[32m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model llama3 not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\n",
      "Model: llama3\n",
      "Tokens per message: 3\n",
      "Tokens per name: 1\n",
      "[autogen.oai.client: 07-27 04:39:08] {329} WARNING - Model llama3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mEngineer\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_ad302309-b3a7-4bb6-9f04-7e8a1090dba0): calculate_daily_spread *****\u001b[0m\n",
      "Arguments: \n",
      "{\"directory\": \"CDR.WA\"}\n",
      "\u001b[32m***************************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "You will need to improve app in FastApi. For now, check out all the application files, try to understand it and wait for next instructions.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6dc05-b1fc-4c1d-b101-4e91dfa63b43",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Finally, our agents modified a code so it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec75d5-035d-4cd6-956e-cafb37f304e7",
   "metadata": {},
   "source": [
    "```python\n",
    "# backend_dir/main.py\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from spread_calculation import calculate_daily_spread\n",
    "import yfinance as yf\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/compare_daily_spread\")\n",
    "async def compare_daily_spread():\n",
    "    cdr_spread = calculate_daily_spread(\"CDR.WA\")\n",
    "    bits_spread = calculate_daily_spread(\"11B.WA\")\n",
    "    spread_difference = cdr_spread - bits_spread\n",
    "    if spread_difference > 0:\n",
    "        return {'message': 'CD Project Red has a larger daily spread', 'difference': spread_difference}\n",
    "    elif spread_difference < 0:\n",
    "        return {'message': '11bits Studio has a larger daily spread', 'difference': -spread_difference}\n",
    "    else:\n",
    "        return {'message': 'Both stocks have the same daily spread', 'difference': 0}\n",
    "\n",
    "\n",
    "# backend_dir/spread_calculation.py\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "def calculate_daily_spread(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    today_data = stock.history(period=\"1d\")\n",
    "    spread = ((today_data[\"High\"] - today_data[\"Low\"]) / today_data[\"Low\"]) * 100\n",
    "    return spread.values[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1dc47-53b7-417f-af8b-f8c4d73c1d7c",
   "metadata": {},
   "source": [
    "You can check out work of application with Postman or curl and see the next output:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52418e-9a67-4ea2-984e-5a14bdd78255",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"message\": \"11bits Studio has a larger daily spread\",\n",
    "    \"difference\": 1.7968083865943187\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Equip your agent with functions that can efficiently implement features into your software application.",
   "tags": [
    "function call",
    "code generation",
    "tool use",
    "software engineering"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
